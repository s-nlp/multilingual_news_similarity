{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 30 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tqdm/std.py:651: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from utils.utils import load_and_prepare_data, prepare_for_training, preprocess\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers = 30, progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface embeddings + cossim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.transformers.HF_emb_COSSIM.emb_distance import emb_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = ['loaded_data_cleaned.csv', 'loaded_data_2_cleaned.csv']\n",
    "data = load_and_prepare_data(data_labels, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_for_training(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['distilbert-base-multilingual-cased', 'bert-base-multilingual-cased', 'bert-base-multilingual-uncased',\n",
    "          'xlm-mlm-xnli15-1024', 'xlm-roberta-base', 'xlm-roberta-large', 'facebook/m2m100_418M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "for model in tqdm(models):\n",
    "    print(f\"Start {model}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    try:\n",
    "        results[model] = emb_distance(model, data, device = device)\n",
    "        print(\"Corr: \", results[model][\"correlation\"])\n",
    "        with open(\"./results/results_trans_part2.pickle\", \"wb\") as f:\n",
    "            pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface embeddings + FC + L2NORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.transformers.HF_emb_FCL2NORM.train import train_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = ['loaded_data_cleaned.csv', 'loaded_data_2_cleaned.csv']\n",
    "data = load_and_prepare_data(data_labels, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_for_training(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"methods/transformers/HF_emb_FCL2NORM/models_configs_HF_FC_L2NORM.pickle\"\n",
    "\n",
    "with open(config_path, \"rb\") as f:\n",
    "    model_configs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['distilbert-base-multilingual-cased', 'bert-base-multilingual-cased', 'bert-base-multilingual-uncased',\n",
    "          'xlm-mlm-xnli15-1024', 'xlm-roberta-base', 'xlm-roberta-large', 'facebook/m2m100_418M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_FC_L2Norm_cosim1 = {}\n",
    "checkpoints_path = \"./checkpoints/HF_FC_L2NORM\"\n",
    "figs_path = \"./figs/HF_FC_L2NORM\"\n",
    "for i, model_name in enumerate(model_configs):\n",
    "    train_eval(model_name, data, model_configs[model_name][\"batch_size\"], model_configs[model_name][\"batch_size_val\"], \n",
    "               model_configs[model_name][\"linear_layer_size\"], model_configs[model_name][\"num_epoch\"],\n",
    "               result_FC_L2Norm_cosim1, train = False, checkpoints_path = checkpoints_path, figs_path = figs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface embeddings + FC + Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.transformers.HF_emb_FCReg.train import train_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = ['loaded_data_cleaned.csv', 'loaded_data_2_cleaned.csv']\n",
    "data = load_and_prepare_data(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_for_training(data, method = \"HF_emb_FCReg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"methods/transformers/HF_emb_FCReg/models_configs_HF_emb_FCReg.pickle\"\n",
    "\n",
    "with open(config_path, \"rb\") as f:\n",
    "    model_configs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['distilbert-base-multilingual-cased', 'bert-base-multilingual-cased', 'bert-base-multilingual-uncased',\n",
    "          'xlm-mlm-xnli15-1024', 'xlm-roberta-base', 'xlm-roberta-large', 'facebook/m2m100_418M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_HF_emb_FCReg = {}\n",
    "checkpoints_path = \"./checkpoints/HF_emb_FCReg\"\n",
    "figs_path = \"./figs/HF_emb_FCReg\"\n",
    "for i, model_name in enumerate(model_configs):\n",
    "    train_eval(model_name, data, model_configs[model_name][\"batch_size\"], model_configs[model_name][\"batch_size_val\"], \n",
    "               model_configs[model_name][\"linear_layer_size\"], model_configs[model_name][\"num_epoch\"],\n",
    "               result_HF_emb_FCReg, train = False, checkpoints_path = checkpoints_path, figs_path = figs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.NLI.Basic.extractor import nli_extractor\n",
    "from methods.NLI.Basic.train import train_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = ['loaded_data_cleaned.csv', 'loaded_data_2_cleaned.csv']\n",
    "data = load_and_prepare_data(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_for_training(data, method = \"NLI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model_name = 'joeddav/xlm-roberta-large-xnli'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 1,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 0\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7582it [02:40, 47.14it/s]\n"
     ]
    }
   ],
   "source": [
    "df_nli = nli_extractor(data, model, tokenizer, params, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nli = pd.read_csv(\"./checkpoints/NLI/nli_scores.csv\", index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nli = data.join(nli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression correlation: 0.09630289373511501\n",
      "Lasso correlation: 3.4153926468658365e-16\n",
      "Ridge correlation: 0.09398195676364932\n",
      "ElasticNet correlation: 3.4153926468658365e-16\n",
      "DecisionTreeRegressor correlation: 0.6086772422091575\n",
      "KNeighborsRegressor correlation: 0.32554466721559555\n",
      "GradientBoostingRegressor correlation: 0.71488954402111\n"
     ]
    }
   ],
   "source": [
    "res = train_eval(df_nli)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tqdm/std.py:651: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "from methods.NER.ner_extractor import NerExtractor\n",
    "from methods.NER.score_calculator import ScoreCounter\n",
    "from methods.NER.train import train_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from polyglot.text import Text\n",
    "import spacy \n",
    "\n",
    "from tqdm import tqdm,trange\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_NER_methods = [\"Huggingface\", \"Polyglot\", \"Spacy\"]\n",
    "method = \"Huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = ['loaded_data_cleaned.csv', 'loaded_data_2_cleaned.csv']\n",
    "if method == \"Huggingface\":\n",
    "    data = load_and_prepare_data(data_labels, preprocess_func = preprocess)\n",
    "else:\n",
    "    data = load_and_prepare_data(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerExtractor = NerExtractor(method = method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 164/7582 [00:29<08:55, 13.85it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "# we can also extract one vector of NERs here with nerExtractor.extract1ner\n",
    "if method == \"Huggingface\":\n",
    "    data[\"ner1\"] = data[[\"url1_lang\", \"content1\"]].progress_apply(lambda x: nerExtractor.extract3ner(x[1], text_lang = x[0]), axis=1)\n",
    "    data[\"ner2\"] = data[[\"url2_lang\", \"content2\"]].progress_apply(lambda x: nerExtractor.extract3ner(x[1], text_lang = x[0]), axis=1)\n",
    "else:\n",
    "    data[\"ner1\"] = data[[\"url1_lang\", \"content1\"]].parallel_apply(lambda x: nerExtractor.extract3ner(x[1], text_lang = x[0]), axis=1)\n",
    "    data[\"ner2\"] = data[[\"url2_lang\", \"content2\"]].parallel_apply(lambda x: nerExtractor.extract3ner(x[1], text_lang = x[0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./checkpoints/NER/Polyglot_ner_2parts.pickle\", \"rb\") as f:\n",
    "    ners = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ner1'] = ners['ner1']\n",
    "data['ner2'] = ners['ner2']\n",
    "keys = ['LOC', 'PER', 'ORG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttext \n",
    "scoreCounter = ScoreCounter(loadFastText = True, ft_models_path = './fasttext_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in keys:\n",
    "    data[\"sim_\" + key] = data[[\"ner1\", \"ner2\", \"url1_lang\", \"url2_lang\"]].progress_apply(lambda x: scoreCounter.fasttext_scores(x[0][key], x[1][key],\n",
    "                                                                                             x[2], x[3]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7582/7582 [00:00<00:00, 173908.26it/s]\n",
      "100%|██████████| 7582/7582 [00:00<00:00, 202498.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocabulary...\n",
      "Created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#tf-idf\n",
    "scoreCounter = ScoreCounter(needVocab = True, data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7582/7582 [00:02<00:00, 3720.30it/s]\n",
      "100%|██████████| 7582/7582 [00:02<00:00, 3049.51it/s]\n",
      "100%|██████████| 7582/7582 [00:01<00:00, 4000.79it/s]\n"
     ]
    }
   ],
   "source": [
    "for key in keys:\n",
    "    data[\"sim_\" + key] = data[[\"ner1\", \"ner2\"]].progress_apply(lambda x: scoreCounter.tf_idf_scores(x[0][key],x[1][key], key), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading transformer model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "#transformers\n",
    "scoreCounter = ScoreCounter(loadTransformers = True, hf_model_name = \"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 1882/7582 [00:50<02:05, 45.41it/s]"
     ]
    }
   ],
   "source": [
    "for key in keys:\n",
    "    data[\"sim_\" + key] = data[[\"ner1\", \"ner2\"]].progress_apply(lambda x: scoreCounter.transformers_scores(x[0][key], x[1][key]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression correlation: 0.25716816501388384\n",
      "Lasso correlation: 2.1396889616053544e-16\n",
      "Ridge correlation: 0.257168566802717\n",
      "ElasticNet correlation: 2.1396889616053544e-16\n",
      "DecisionTreeRegressor correlation: 0.6479410747621763\n",
      "KNeighborsRegressor correlation: 0.4105426013716557\n",
      "GradientBoostingRegressor correlation: 0.47861064193091774\n"
     ]
    }
   ],
   "source": [
    "res = train_eval(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
